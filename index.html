<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Matthew Lisondra</title>
  
  <meta name="author" content="Matthew Lisondra">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/matthew_lisondra_icon.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Matthew Lisondra</name>
              </p>
              <p>I am currently a Researcher of Robotics at the <a href="https://robotics.utoronto.ca/">University of Toronto Robotics Institute</a>.
              </p>
              <p>
                 My research focuses on Robot Perception, Robot Learning, Computer Vision, Simultaneous Localization and Mapping (SLAM), Autonomous/Intelligent Systems Algorithms, High Framerate Processing Low-Power Unconventional Sensing, 3D Scene Representations and Embodied AI.
              </p>
              <heading>Research Affiliations</heading>
              <p>
                I am affiliated with the following research labs:
              </p>

              <ul>
                <li><a href="http://asblab.mie.utoronto.ca/">Autonomous Systems and Biomechatronics Lab (ASB)</a> directed by Dr. G. Nejat</li>
                <li><a href="https://sajad-saeedi.ca/">Robotics and Computer Vision Laboratory (RCVL)</a> directed by Dr. S. Saeedi</li>
                <li><a href="https://www.torontomu.ca/mechanical-industrial-mechatronics/research/#!accordion-1574283287653-haptics-and-telerobotics-laboratory--haptel-">Haptics and Telerobotics Laboratory (HapTel)</a> directed by Dr. K. Zareinia</li>
                <li><a href="https://ingenuitylabs.queensu.ca/">Ingenuity Labs Research Institute</a> (with Dr. A. Wu)</li>

            </ul>
              <p style="text-align:center">
                <a href="mailto:matthew.lisondra@alum.utoronto.ca">Email</a> &nbsp;/&nbsp;
                <a href="data/Matthew_Lisondra_CV.pdf"> Curriculum Vitae (CV)</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/mattlisondra">LinkedIn</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=IQqguaIAAAAJ&hl=en">Google Scholar</a> 
                <!-- &nbsp;/&nbsp;
                <a href="https://www.researchgate.net/profile/Matthew-Lisondra">Research Gate</a> &nbsp;/&nbsp;
                <a href="https://github.com/mattlisondra">GitHub</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/matthew_lisondra.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>              
              <p>
                <a href="https://robotics.utoronto.ca/">UofT Robotics Institute</a>, <a href="https://www.utoronto.ca/">University of Toronto</a> PhD, Doctor of Philosophy (Mechanical Engineering)
              </p>
              <ul>
                  <li>Focus: Robot Perception, Robot Learning, Computer Vision</li>
                  <li>Leveraging LLM and VLM agents w/ Computer Vision for Robot Learning</li>
                  <li>Research at the <a href="https://robotics.utoronto.ca/">University of Toronto Robotics Institute</a></li>
                  <li>Supervised by: <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Dr. G. Nejat</a> of the <a href="http://asblab.mie.utoronto.ca/">ASB Lab</a></li>
              </ul>
              <p>
                <a href="https://sajad-saeedi.ca/">Robotics and Computer Vision Laboratory (RCVL)</a> MASc, Masterâ€™s of Applied Science (Mechanical Engineering)
              </p>
              <ul>
                  <li>Focus: Computer Vision and Visual-Inertial SLAM</li>
                  <li>Designed the first 6-DOF Visual Inertial Odometry on FPSPs (BIT-VIO)</li>
                  <li>Thesis Published, Presentation in <a href="https://2024.ieee-icra.org/">Yokohama, Japan for IEEE 2024 ICRA</a></li>
                  <li>Collaboration with Imperial College London and University of Manchester</li>
                  <li>Supervised by: <a href="https://www.torontomu.ca/mechanical-industrial-mechatronics/people/faculty/sajad-saeedi/">Dr. S. Saeedi</a> of <a href="https://sajad-saeedi.ca/">RCVL</a>, <a href="https://www.torontomu.ca/mechanical-industrial-mechatronics/people/faculty/kourosh-zareinia/">Dr. K. Zareinia</a> of <a href="https://www.torontomu.ca/mechanical-industrial-mechatronics/research/#!accordion-1574283287653-haptics-and-telerobotics-laboratory--haptel-">HapTel Lab</a></li>
              </ul>
              <p>
                <a href="https://www.physics.utoronto.ca/">Physics</a>, <a href="https://www.utoronto.ca/">University of Toronto</a> HBSc, Honours Bachelor of Science (Physics and Computer Science)
              </p>
              <ul>
                  <li>Focus: Robotic Mechanics, Probability, TS-Analysis, Computational Physics</li>
                  <li>Research: Time Series Analysis on Global Temperature, Sea Level Pressure</li>
                  <li>Research: Helium-Neon Laser Analysis (Reviewed by <a href="https://www.physics.utoronto.ca/~vutha/">Dr. A. Vutha</a>)</li>
                  <li>Research: Percolation via Random Processes Monte Carlo,  Porous Rock</li>
                  <li>Collaborated with: <a href="https://www.physics.utoronto.ca/members/jones-dylan/">Dr. D. Jones</a> of <a href="https://jones-group.physics.utoronto.ca/">APCM Group</a></li>
              </ul>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Publications/Works</heading>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2025_ram_embodiedai_foundation_models.png" alt="secure" width="200" height="202">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>[1] Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review</papertitle>
            </a>
            <br>
            <strong>Matthew Lisondra1</strong>,
            Beno Benhabib1,
            <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Goldie Nejat1</a>
            <br>
            (1University of Toronto)
            <br>
            <em>IEEE Robotics and Automation Magazine (RA-M)</em>, 2025
            <br>
            <em>Special Issue on Embodied AI: Bridging Robotics and Artificial Intelligence Toward Real-World Applications</em>
            <br>
            (In Review)
            <br>
            <p></p>
            <p>
              Presents the first systematic review of foundation models integrated into mobile service robotics
              <br>
              Identifies and categorizes the core challenges in embodied AI: multimodal sensor fusion, real-time decision-making, task generalization, and effective human-robot interaction
              <br>
              Analyzes how foundation models (LLMs, VLMs, MLLMs, VLAs) enable real-time sensor fusion, language-conditioned control, and adaptive task execution
              <br>
              Examines real-world deployments across domestic assistance, healthcare, and service automation domains
              <br>
              Demonstrates the transformative role of foundation models in enabling scalable, adaptable, and semantically-grounded robot behavior
              <br>
              Proposes future research directions emphasizing predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization
              <br>
              Highlights the need for robust and efficient deployment of foundation models in human-centric robotic systems
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2025_ram_embodiedai_foundation_models.png" alt="secure" width="200" height="202">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>[2] TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry</papertitle>
            <br>
            <strong>Matthew Lisondra*1</strong>,
            <a href="https://kimjunseo.com/">Junseo Kim*2</a>,
            Glenn Takashi Shimoda3,
            <a href="https://sajad-saeedi.ca/">Sajad Saeedi4</a>
            <br>
            (1University of Toronto, 2TU Delft, 3TMU, 4University College London)
            <br>
            <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2025
            <br>
            (In Review)
            <br>
            <p></p>
            <p>
              Presents TCB-VIO: the first tightly coupled 6-DoF visual-inertial odometry algorithm designed for Focal-Plane Sensor-Processor Arrays (FPSPs)
              <br>
              Achieves high-speed visual-inertial fusion at 250 FPS (vision) and 400 Hz (IMU) using a Multi-State Constraint Kalman Filter (MSCKF)
              <br>
              Introduces a novel binary-enhanced Kanade-Lucas-Tomasi (KLT) tracker tailored for on-sensor binary edge images and feature maps
              <br>
              Leverages FPSP on-sensor processing to drastically reduce data transfer latency and power consumption for mobile robotics
              <br>
              Maintains accurate and robust tracking even under aggressive motion where state-of-the-art methods (VINS-Mono, ORB-SLAM3, ROVIO) fail
              <br>
              Demonstrates superior tracking smoothness and feature longevity using long binary feature tracks in real-world environments
              <br>
              Validated on the SCAMP-5 FPSP, highlighting robustness despite analog computing constraints such as limited per-pixel memory and noise
              <br>
              Extends and adapts the OpenVINS tightly coupled framework to work with binary feature-based, low-power embedded vision sensors
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2025_ram_embodiedai_foundation_models.png" alt="secure" width="200" height="202">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>[3] High-frequency Focused Ultrasound for Microplastics Identification and Size Estimation</papertitle>
            </a>
            <br>
            <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Navid Zarrabi2</a>,
            <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Eric Stohm3</a>,
            <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Hadi Rezvani4</a>,
            <strong>Matthew Lisondra1</strong>,
            <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Nariman Yousefi5</a>,
            <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Sajad Saeedi1</a>,
            <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Michael Kolios6</a>
            <br>
            (1University of Toronto)
            <br>
            2025
            <br>
            (In Review)
            <br>
            <p></p>
          <p>
            Proposes a novel ultrasound-based method for detecting, identifying, and sizing microplastic particles using high-frequency acoustic signals
            <br>
            Introduces a peak extraction technique to isolate key features from backscattered ultrasound signals for improved material classification
            <br>
            Develops a 1-D Convolutional Neural Network (CNN) achieving 99.63% accuracy for classifying microplastic materials (PE, PMMA, steel, glass)
            <br>
            Implements a two-hidden-layer material-specific Multilayer Perceptron (MLP) that estimates particle size with 99.97% accuracy
            <br>
            Demonstrates an integrated machine learning pipeline combining material identification and size estimation from ultrasound signals
            <br>
            Offers a low-cost, rapid, and automated alternative to traditional microplastic detection techniques (e.g., Raman, FTIR, microscopy)
            <br>
            Validates the approach on a custom dataset of microsphere backscatter signals, demonstrating potential for in-situ environmental monitoring
            <br>
            Advances the field of acoustic signal processing by showcasing the potential of ultrasound and AI for environmental pollutant characterization
          </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2025_ram_embodiedai_foundation_models.png" alt="secure" width="200" height="202">
          </td>
          <td width="75%" valign="middle">
            <a href="">
              <papertitle>[4] PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living</papertitle>
            </a>
            <br>
            <strong>Matthew Lisondra*1</strong>,
            Souren Pashangpour*1,
            <a href="https://www.revolvesurgical.com/">Fraser Robinson2</a>
            <a href="https://www.mie.utoronto.ca/faculty_staff/nejat/">Goldie Nejat1</a>
            <br>
            (1University of Toronto, 2Resolve Surgical)
            <br>
            <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2025
            <br>
            (In Review)
            <br>
            <p></p>
            <p>
              Presents the first multimodal deep learning architecture for autonomous multi-activity recognition in socially assistive robots (SARs)
              <br>
              Introduces a novel ADL and motion embedding space to distinguish between known ADLs, unseen ADLs, and atypical ADL performances
              <br>
              Proposes a new user state estimation method that applies similarity functions to identify novel or atypical ADLs in real-time
              <br>
              Enables proactive human-robot interaction (HRI) by allowing SARs to autonomously initiate assistive behaviors based on activity context
              <br>
              Addresses limitations of prior ADL recognition systems which misclassified non-ADL movements and lacked online generalization
              <br>
              Demonstrates higher ADL classification accuracy compared to state-of-the-art human activity recognition models
              <br>
              Validates the system through real-world HRI experiments with the socially assistive robot Leia in cluttered living environments
              <br>
              Supports diverse users and environments by recognizing a wider range of ADLs and adapting to personalized or atypical behaviors
            </p>
          </td>
        </tr>
          
          
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/sfwgif3.gif" alt="secure" width="200" height="202">
          </td>
          <td width="75%" valign="middle">
            <a href="https://sites.google.com/view/structure-from-wifi/journal-extension">
              <papertitle>[5] Inverse k-visibility for RSSI-based Indoor Geometric Mapping (In Review)</papertitle>
            </a>
            <br>
            <strong>Matthew Lisondra*1</strong>,
            <a href="https://kimjunseo.com/">Junseo Kim*2</a>,
            <a href="https://www.cs.torontomu.ca/~bahoo/">Yeganeh Bahoo3</a>,
            <a href="https://sajad-saeedi.ca/">Sajad Saeedi3</a>
            <br>
            (1University of Toronto, 2TU Delft, 3TMU)
            <br>
            <em>IEEE Sensors Journal (ISJ)</em>, 2024
            <br>
            <em>Special Issue on Machine Learning for Radio Frequency Sensing</em>
            <br>
            (In Review)
            <br>
            <a href="data/KimLisondra2024arXiv.bib">bibtex</a>
            /
            <a href="https://sites.google.com/view/structure-from-wifi/journal-extension">Project Webpage</a>
            /
            <a href="https://arxiv.org/abs/2408.07757">PDF</a>
            <br>
            <p></p>
            <p>
              Presents a novel technique capable of generating geometric maps from WiFi signals
              <br>
              A novel algorithm that is capable of generating geometric maps using WiFi signals received from multiple routers
              <br>
              Benchmarking the WiFi-generated maps with Lidar-generated maps by comparing the area,
              <br>
              number of data points, RSSI prediction True/False setting, RSSI accuracy percentage, IOU and MSE scores
              <br>
              Evaluation on real-world collected from indoor spaces
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2024_icra_bitvio.png" alt="secure" width="200" height="202">
          </td>
          <td width="75%" valign="middle">
            <a href="https://sites.google.com/view/bit-vio/home">
              <papertitle>[6] Visual Inertial Odometry using Focal Plane Binary Features (BIT-VIO)</papertitle>
            </a>
            <br>
            <strong>Matthew Lisondra*1</strong>,
            <a href="https://kimjunseo.com/">Junseo Kim*2</a>,
            <a href="https://rmurai.co.uk/">Riku Murai3</a>,
            Koroush Zareinia4,
            <a href="https://sajad-saeedi.ca/">Sajad Saeedi4</a>
            <br>
            (1University of Toronto, 2TU Delft, 3Imperial College London, 4TMU)
            <br>
            <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024
            <br>
            <a href="data/Lisondra2024arXiv.bib">bibtex</a>
            /
            <a href="https://sites.google.com/view/bit-vio/home">Project Webpage</a>
            /
            <a href="https://arxiv.org/abs/2403.09882">PDF</a>
            /
            <a href="https://youtu.be/fPKUfz9PFeY?si=ql6OnsAB2MCwi_wR">Video</a><br /> 
            <br>
            <p></p>
            <p>
              Designed the first 6-DOF Visual Inertial Odometry on FPSPs (BIT-VIO)
              <br>
              Efficient VIO operating and correcting by loosely-coupled sensor-fusion iEKF at 300 FPS 
              <br>
              using predictions from IMU measurements obtained at 400 Hz
              <br>
              Uncertainty propagation for BIT-VO's pose as it is based on binary-edge-based descriptor extraction
              <br>
              Extensive real-world comparison against BIT-VO, with ground-truth obtained using a motion capture system
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/MASc_Thesis_Matthew_Lisondra_Final.png" alt="secure" width="200" height="202">
          </td>
          <td width="75%" valign="middle">
            <a href="data/MASc_Thesis_Matthew_Lisondra_Final.pdf">
              <papertitle>[7] Focal-Plane Sensor-Processor-Based Visual Inertial Odometry</papertitle>
            </a>
            <br>
            <strong>Matthew Lisondra*1</strong>,
            <br>
            (1University of Toronto)
            <br>
            <em>Thesis</em>, 2024
            <br>
            <a href="data/MASc_Thesis_Matthew_Lisondra_Final.bib">bibtex</a>
            /
            <a href="data/MASc_Thesis_Matthew_Lisondra_Final.pdf">PDF</a>
            <br>
            <p></p>
            <p>
              Studied the usability and advantages of FPSPs to leverage a more accurate state estimation framework
              <br>
              Designed an algorithm for VIO using Focal-Plane Binary Features 
              <br>
              Implemented the FPSP vision- IMU-fused estimation algorithm on a mobile device for offline and online real-world testing
              <br>
              Evaluate the performance, benchmarking against FPSP vision-alone and ground-truth data
              <br>
              Extensive study on the algorithmic execution timing/frame, accuracy, memory usage
              <br>
              and power consumption of the visual front-end processing performance on the FPSP
            </p>
          </td>
        </tr>

          
              </p>
            </td>
          </tr>
    
        </tbody></table>

        <br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Organizations</heading>              
              <ul>
                  <li>Reviewer (Journal) for ISJ 2025, IEEE Sensors Journal (ISJ) 2025</li>
                  <li>Reviewer (Journal) for RA-L 2024, IEEE Robotics and Automation Letters (RA-L) 2024</li>
                  <li>Reviewer (Conference) for IROS 2024, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</li>
                  <li>Reviewer (Conference) for ICRA 2024, International Conference on Robotics and Automation (ICRA) 2024</li>
                  <li>Reviewer (Conference) for IEEE CCECE 2023, 2023 Canadian Conference On Electrical and Computer Engineering</li>
                  <li>Reviewer (Conference) for IROS 2023, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>              
              <p>
                I am affiliated with several teaching institutions, teaching Physics (Advanced) and Computer Science. Select students I have taught:
              </p>
              <ul>
                  <li>H. Li</li>
                  <li>L. Liang</li>
                  <li>N. Feng</li>
                  <li><b>R. Sun*</b> <b>(Multiple Scholarship Offers in Australia, England)</b></li>
                  <li>W. Zheng</li>
                  <li>A. Zhang</li>
                  <li><b>A Huang*</b> <b>(Now at Berkeley Music School)</b></li>
                  <li>B. Zheng</li>
                  <li><b>B. Luo*</b> <b>(Full Scholarship Physics, University College London)</b></li>
                  <li><b>B. Yang*</b> <b>(Now at King's College London)</b></li>
                  <li><b>C. Yang*</b> <b>(Now at Durham University)</b></li>
                  <li>F. Chen</li>
                  <li><b>G. Ye*</b> <b>(Now at University of Hong Kong)</b></li>
                  <li>J. Wu</li>
                  <li><b>K. Sheng, Chuwen</b> <b>(Multiple University Offers)</b></li>
                  <li>L. Lin</li>
                  <li><b>N. Zhao*</b> <b>(Multiple University Offers in Business)</b></li>
                  <li>N. Tian</li>
                  <li>A. Pu</li>
                  <li>D. Qiu</li>
                  <li>M. Wang</li>
                  <li>R. Zhang</li>
                  <li>C. Zou</li>
                  <li>Y. Caelon</li>
                  <li>S. Yumo</li>
                  <li>A. Ma</li>
                  <li>A. Xu</li>
                  <li>K. Chen</li>
                  <li>F. Xin</li>
                  <li>A. Yi Lan</li>
                  <li>C. Yi</li>
                  <li>F. Chen</li>
                  <li>L. Li</li>
                  <li>M. Di</li>
                  <li>R. Zhang</li>
                  <li>S. Fan</li>
              </ul>
              <p>
              <b>Thesis Mentoring/Guidance:</b> 
              <a href="https://kimjunseo.com/">Junseo Kim</a> (Now at TUDelft Robotics)
              </p>
              <p>
              <a href="https://kimjunseo.com/publication/jason-capstone/Final_Report_Capstone.pdf">Autonomous Truck Navigation with Trailer Integration via Natural Language Processing (NLP)</a>
              </p>
              <p>
                More information as well as students taught can be found on my <a href="data/Matthew_Lisondra_CV.pdf"> Curriculum Vitae (CV)</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Industry </heading>              
                <p>
                  <b><a href="https://www.rosor.ca/">Rosor Exploration</a></b>
                </p>
                <ul>
                    <li>Researcher - Robotics, Geoscientific UAVs and Drones</li>
                    <li>Working on Development of Rosor's Active Terrain Following (ATR) System</li>
                    <li>Currently: Now Led by <a href="https://h2jaafar.github.io/"> H. A. Jafaar</a></li>
                </ul>
                <br>
                <br>
                <br>
                <p>
                Outside of Research, I do Filmmaking with Film Director <a href="https://dinithavithanage.com/about/">Dinitha Vithanage</a> and Film Editor <a href="https://www.brandonworden.com/">Brandon Worden</a>. We just recently presented <a href="film.html">our Feature-Film</a>.
                </p>
            </td>
          </tr>
        </tbody></table>
  
        <table style="width:25%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:25px;width:25%;vertical-align:middle">
            <p style="text-align:center;font-size:small;">
              <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=JtIJAyYcKr3dN0XGKJD8DsvksycEk6OrzU4afthpWrw&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
              <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=JtIJAyYcKr3dN0XGKJD8DsvksycEk6OrzU4afthpWrw"></script>
            </p>
          </td>
        </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
  
</body>

</html>
